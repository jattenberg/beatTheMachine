\section{Beat the Machine}
\label{sec:btm}
\josh{ensure that the language matches the earlier motivation in secs 2 and 3}
Assessing the in-the-wild performance of any automated classification system can be challenge. Situations with class imbalance and rare disjunctive sub-concepts such as the hate speech classifier presented in Section~\ref{sec:intro} makes accurate assessment particularly difficult. Traditionally, we would sample from the output decisions and employ humans to verify the correctness of the classifications.  Using these judgments we can estimate the error rate. Unfortunately, given our problem characteristics, this process can be woefully inefficient. First, if the classification decisions are relatively accurate, then most of the results will be accurate, and without intelligent sampling, humans will encounter errors very infrequently. Second, if there is substantial class imbalance, most of the encountered errors would be misclassifications of examples truly of the majority class into the minority. This is problematic since in significantly imbalanced classification problems, the minority class must necessarily incur a far greater mistake cost--- as in the case of hate speech, this is what is being predicted. Third, if the problem space has rare disjunctive sub-concepts, identification may be particularly tricky--- chances of occurrence may be $1:1,000,000$ or less. In these situations, it can become quite difficult to identify misclassifications of examples truly in the minority class. 

\begin{xmpl} Consider the case of identifying pages with hate speech content. In   reality, less than 0.1\% of the pages on the Internet contain such content. If we   have a relatively accurate classifier, with 95\% error rate on each class, it becomes very difficult to identify misclassified pages that contain hate speech. In a random sample, most of the pages are correctly classified as benign. To find one ``false negative'' (the severe error: hate speech passing as benign) we will have to inspect approximately $20,000$ pages (and in the process would find around $1,000$ false positives). This is echoed in the   performance of the ``random labeling'' used to generate the $k$-NN model presented in Figure~\ref{fig:uuvsku}. \end{xmpl} 

It is tempting to consider such problems inconsequential. However, when such a system is used to filter billions of pages, such ``relatively infrequent'' errors become frequent in absolute numbers. Furthermore, even isolated, ``outlier'' cases can cause significant damage, for example, to the public image of a company that accidentally supports a site containing such content through advertising. Unknown unknowns may be particularly damaging; client's expectations haven't been properly managed, and detailed contingencies are unlikely to exist. 

Instead of passively waiting for such unknown errors to ``emerge'' we can instead actively seek to find them. In a sense, this is similar to ``white hat'' hackers that are hired by companies to find vulnerabilities and break into their own security systems. In our case, human workers are asked to submit pages that will ``beat'' our classifier.

\drop{
old version
Assessing and improving the quality of an automatic classification system is
challenging in environments with the characteristics listed above. 
Traditionally, we would sample from the output decisions and employ humans to verify the correctness of the classifications.  Using these judgments we can estimate the error rate.  Unfortunately, given our problem characteristics, this process can be woefully inefficient. First, if the classification decisions are relatively accurate, then most of the results will be accurate, and without 
intelligent sampling, humans will encounter errors very infrequently.  Second, if there is class imbalance, ceteris paribus, most of the encountered errors would be misclassifications of examples truly of the majority class into the minority, rather than less common (and often disjunctive) minority regions being improperly labeled.  If both of these conditions hold, then it becomes quite difficult to identify misclassifications of examples truly of the minority class.

\begin{xmpl} Consider the case of identifying pages with hate speech content. In   reality, less than 0.1\% of the pages on the Internet contain such content. If we   have a relatively accurate classifier, with 95\% error rate on each class, it becomes very difficult to identify misclassified pages that contain hate speech. In a random sample, most of the pages are correctly classified as benign. To find one ``false negative'' (the severe error: hate speech passing as benign) we will have to inspect approximately $20,000$ pages (and in the process would find around $1,000$ false positives). \end{xmpl}

It is tempting to consider such problems inconsequential. However, when such a system is used to filter billions of pages, such ``relatively infrequent'' errors become frequent in absolute numbers. Furthermore, even isolated, ``outlier'' cases can cause significant damage, for example, to the public image of a company that accidentally supports a site containing such content through advertising.

Instead of passively waiting for such errors to ``emerge'' we can instead actively seek to find them. In a sense, this is similar to ``white hat'' hackers that are hired by companies to find vulnerabilities and break into their own security systems. In our case, human workers are asked to submit pages that will ``beat'' our classifier.

The selective acquisition of example labels with the intent of building robust performance estimators at minimal cost is a topic getting recent attention in the research literature~\cite{activerisk2010,bennett:online}. However, while promising and potentially useful in practice, such acquisition strategies are focused on minimizing the cost required to compute a robust estimator for precision or total loss. In order to construct such an estimator, existing selective acquisition strategies sample from the problem space in accordance to some function of the output score of the model being considered. However, given a capable model deployed in a production system, it may take millions of samples from high-confidence positive predictions to reveal a single example that ``beats the machine.'' Incorporating performance bounds such as those presented in the referenced research with our proposed selection strategy is an interesting direction for future work. 
}

\subsection{BTM Task Design}

To describe the design of Beat the Machine, we now will walk through several designs of increasing sophistication, building up the ideas by focusing on challenges and subsequently more sophisticated designs.

\textbf{Design 1}: Let's start with a straightforward idea: Ask humans to find cases that ``beat the machine''---the users would submit URLs that they believed would be incorrectly classified by the current classification model.  To spur engagement, a user would receive a nominal payment for just submitting the URLs, and then she would receive a significant bonus payment for every URL that was misclassified. (In the implementation, the nominal payment was 1 cent per 5 URLs, and the payment per misclassified URL was a maximum of 50 cents.)  To judge the misclassification, we asked other (trusted) humans to classify these URLs, and then to determine whether the URL beat the machine, we compared the outcome of the trusted human classification with the outcome of the machine model. To avoid certain issues of gaming, the BTM workers were recruited through Amazon Mechanical Turk, and the trusted human judges were recruited and trained through oDesk for the fully automated system, and were student interns using a separate system for the experimental evaluation below.  Unfortunately, this simple design was not as effective as we would have liked, for a variety of reasons.

The first, and most obvious, problem that we encountered was the lack of interactivity.  The workers could easily submit URLs that would break the model, but then they had to wait for other humans to inspect the results, in order to assess whether they had succeeded. This process would take from a few minutes to a few hours. The delay made the task opaque to the players of the BTM game,
as they did not know if they were ``playing the game'' well or not.

\textbf{Adding immediate classification feedback}: To resolve (partially) the lack of interactivity, we augmented the system to classify URLs on the fly, and give immediate feedback to the humans about the classifier outcome. (For example ``The machine believes that this URL contains hate speech.  Do you believe that this is correct?'') The BTM player could then decide whether the URL was indeed a misclassification case and submit it for further consideration. Upon submission, the user received provisional bonus points that correspond to a cash reward. The bonus points became permanent and the worker was paid immediately after inspection and verification of the submitted content by the human judges.
  

Unfortunately, this design did not provide the proper incentives. Players found it much easier to locate pages from the majority class (e.g., pages without any hate speech content) that would be misclassified as containing hate speech. So, instead of locating the desired, severe infrequent errors, we received the type of errors that we could find more easily by observing the positive classifications.  (Recall that due to the class imbalance, most of the observed errors would be good pages being classified as containing hate speech.) As described above, we are particularly interested in finding pages that contain hate speech but are incorrectly classified as benign.  (And especially, among these, the ``unknown unknowns.'') Furthermore, we experienced a significant number of cheating attempts where users were submitting random URLs and always insisting that the content is different than the classification decisions, even though the classifier was correct.

\begin{figure}[t]
\center{\includegraphics[width=\columnwidth]{plots/btm-HIT.png}}
\vspace{-.3in}
\caption{A screen-shot of the BTM interface on Mechanical Turk.}
\vspace{-.2in}
\label{fig:btm}
\end{figure}

\textbf{Segmenting the task by class}: To deal with these problems, we split the task into two subtasks: (1) Seek pages in the minority class that are misclassified in the majority class (i.e., pages that contain offensive content but are classified as benign), and (2) seek pages with benign content that would be classified as offensive. This segmentation simplified the overall design and made the task easier for participants to understand.  Moreover, it allowed us to quickly reject submissions that were of no interest.  For example, if we are asking for misclassified hate speech pages, we can quickly reject pages that our classifier unambiguously classifies as hate speech. (In the original design, users had the incentive to mark these as ``non-hate-speech'' hoping that the human judge would accept their judgments.) Figure~\ref{fig:btm} shows the (simple) task interface.

\textbf{Expanding the incentives}: In the final design (for this paper) we also improved the incentive structure by rewarding differently users that discover ``big mistakes'' (the ``unknown unknowns'') and those that discover the ``small mistakes'' (the ``known unknowns''). Instead of giving a constant bonus to the player for a misclassified URL, we reward misclassifications proportionally to the confidence of the classifier. 
Examples submitted withing the $\epsilon$ uncertainty boundary of a model are given a 
the reward is small.  This was a known unknown.
On the other hand, if the model is very confident in its decision (i.e., a classification confidence close to 100\%), but the decision is incorrect, then the BTM system gives the highest possible bonus to the worker.\footnote{In our particular implementation, the highest bonus is worth 1000 points, or 50 cents.} If the confidence was lower, say 75\%, then the reward was proportionally smaller. We also reward players that provide examples for which the model was correct but uncertain: if the model predicted that the page is 60\% likely to contain hate speech, and the page indeed contained hate speech, the user received a small bonus.

\josh{reword the last part to better incorporate the strict definition of UU?}

