\section{Beat the Machine}
\label{sec:btm}

% \josh{ensure that the language matches the earlier motivation in secs 2 and 3}

Assessing the in-the-wild performance of any automated classification system can be challenge. Situations with class imbalance and rare disjunctive sub-concepts such as the hate speech classifier presented in Section~\ref{sec:intro} make accurate assessment particularly difficult and lead to the existence of unknown unknowns. Traditionally, we would sample from the output decisions and employ humans to verify the correctness of the classifications.  Using these judgments we can estimate the error rate in different parts of the classification space. Unfortunately, given our problem characteristics, this process can be woefully inefficient. First, if the classification decisions are relatively accurate, then most of the results will be accurate, and without intelligent sampling, humans will encounter errors very infrequently. Second, if there is substantial class imbalance, most of the encountered errors would be misclassifications of examples truly of the majority class into the minority. This is problematic since in significantly imbalanced classification problems, the minority class generally incurs a far greater mistake cost---as in the case of hate speech, this is what is being predicted. Third, if the problem space has rare disjunctive sub-concepts, identification may be particularly tricky---chances of occurrence may be $1:1,000,000$ or less. In these situations, it can become quite difficult to identify misclassifications of examples truly in the minority class. 

\begin{xmpl} Consider the case of identifying pages with hate speech content. If we have a relatively accurate classifier, with 95\% error rate on each class, it becomes very difficult to identify misclassified pages that contain hate speech. In a random sample, most of the pages are correctly classified as benign. Even in the unrealistically generous case that 
0.1\% of the pages on the Internet contain such objectionable content, to find one ``false negative'' (the severe error: hate speech passing as benign) we will have to inspect approximately $20,000$ pages (and in the process would find around $1,000$ false positives). 

%This is echoed in the   performance of the ``random labeling'' used to generate the $k$-NN model presented in Figure~\ref{fig:uuvsku}. 
\end{xmpl} 

 %In reality, far less than 0.1\% of the pages on the Internet contain such content.

It is tempting to consider such problems inconsequential. However, when such a system is used to filter billions of pages, such ``relatively infrequent'' errors become frequent in absolute numbers. Furthermore, even ``outlier'' cases can cause significant damage, for example, to the public image of a company that accidentally supports a site containing such content through advertising. Unknown unknowns may be particularly damaging; client's expectations haven't been properly managed, and detailed contingencies are unlikely to exist. 

Instead of passively waiting for such unknown errors to ``emerge'' we can instead actively seek to find them. We describe a system that engages human intelligence, accessed through crowdsourcing, to identify these ``unknown unknown'' cases. In a sense, this is similar to ``white hat'' hackers that are hired by companies to find vulnerabilities and break into their own security systems. In our case, human workers are asked to submit pages that will ``beat'' our classifier.

\subsection{BTM Task Design}

To describe the design of Beat the Machine, we now will walk through several designs of increasing sophistication, building up the ideas by focusing on challenges and subsequently more sophisticated designs.

\textbf{Design 1}: Let's start with a straightforward idea: Ask humans to find the unknown unknowns, i.e., the cases that ``beat the machine''---the users would submit URLs that they believed would be incorrectly classified by the current classification model.  To spur engagement, a user would receive a nominal payment for just submitting the URLs, and then she would receive a significant bonus payment for every URL that was misclassified. (In the implementation, the nominal payment was 1 cent per 5 URLs, and the payment per misclassified URL was 50 cents.)  

Of course there is an obvious problem: how could such a system tell that the URL indeed beats the machine.  The whole point is to find cases that the system does not know that it gets wrong!   To judge misclassification, we tasked another set of (trusted) humans to classify these URLs.  Then, to determine whether the URL beat the machine, we compared the classification of the trusted set of humans with the outcome of the machine model. To avoid certain issues of gaming, the BTM workers were recruited through Amazon Mechanical Turk, and the trusted human judges were recruited and trained through oDesk for the fully automated system, and were student interns using a separate system for the experimental evaluation.  Unfortunately, this simple design was not as effective as we would have liked, for a variety of reasons.

The first, and most obvious, problem that we encountered was the lack of interactivity.  The workers could easily submit URLs that would break the model, but then they had to wait for other humans to inspect the results, in order to assess whether they had succeeded. This process would take from a few minutes to a few hours. The delay made the task opaque to the players of the BTM game,
as they did not know if they were good in ``playing the game'' or not.

\textbf{Adding immediate classification feedback}: To resolve (partially) the lack of interactivity, we augmented the system to classify URLs on the fly, and give immediate feedback to the humans about the classifier outcome. (For example ``The machine believes that this URL contains hate speech.  Do you believe that this is correct?'') The BTM player could then decide whether the URL was indeed a misclassification case and submit it for further consideration. Upon submission, the user received provisional bonus points that correspond to a cash reward. The bonus points became permanent (and the worker was paid) immediately after the inspection and verification of the submitted content by the human judges.
  

Unfortunately, this design did not provide the proper incentives. Players found it much easier to locate pages from the majority class (e.g., pages without any hate speech content) that would be misclassified as containing hate speech. So, instead of locating the desired, high-cost errors, we received the type of errors that we could find more easily by observing the positive classifications.  (Recall that due to the class imbalance, most of the observed errors would be good pages being classified as containing hate speech.) As described above, we are particularly interested in finding pages that contain hate speech but are incorrectly classified as benign.  (And especially, among these, the ``unknown unknowns.'') Furthermore, we experienced a significant number of cheating attempts where users were submitting random URLs and always insisting that the content is different than the classification decisions, even though the classifier was correct.

\begin{figure}[t]
\center{\includegraphics[width=0.8\columnwidth]{plots/btm-HIT.png}}
\caption{A screen-shot of the BTM interface on Mechanical Turk.}
\label{fig:btm}
\end{figure}

\textbf{Segmenting the task by class}: To deal with these problems, we split the task into two subtasks: (1)~Seek pages in the minority class that are misclassified in the majority class (i.e., pages that contain offensive content but are classified as benign), and (2)~seek pages with benign content that would be classified as offensive. This segmentation simplified the overall design and made the task easier for participants to understand.  Moreover, it allowed us to quickly reject submissions that were of no interest.  For example, if we are asking for misclassified hate speech pages, we can quickly reject pages that our classifier unambiguously classifies as hate speech. (In the original design, users had the incentive to mark these as ``non-hate-speech'' hoping that the human judge would accept their judgments.) Figure~\ref{fig:btm} shows the (simple) task interface.

\textbf{Expanding the incentives}: In the final design , we also improved the incentive structure by rewarding differently users that discover ``big mistakes'' (the ``unknown unknowns'') and those that discover the ``small mistakes'' (the ``known unknowns''). Instead of giving a constant bonus to the player for a misclassified URL, we reward misclassifications proportionally to the estimated misclassification cost, which we infer through the returned confidence.  
Examples that have high estimated misclassification cost are given a the reward is small.  This was a known unknown.
On the other hand, if the model is very confident in its decision (i.e., a classification confidence close to 100\%) and correspondingly has a low estimated misclassification cost, but the decision is incorrect, then the BTM system gives the highest possible bonus to the worker.\footnote{In our particular implementation, the highest bonus is worth 1000 points, or 50 cents.} If the confidence was lower, say 75\%, the estimated misclassification cost was higher, and the reward was proportionally smaller. We also reward players that provide examples for which the model was correct but uncertain: if the model predicted that the page is 60\% likely to contain hate speech, and the page indeed contained hate speech, the user received a small bonus.
