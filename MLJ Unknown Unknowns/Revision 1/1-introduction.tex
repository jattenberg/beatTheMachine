\section{Introduction}
\label{sec:intro}

This paper discusses an important issue for machine learning researchers and practitioners to consider, that has received little if any attention in the machine learning literature, but becomes quite apparent when applying machine learning techniques in practice. How can we know about the vulnerabilities of the model that we have created, and which seems to perform well according to standardized evaluation measures?\footnote{For example, using area-under-the-curve, computed using cross-validation}

We worked with a firm to build systems for identifying web pages that contain instances of objectionable content, such as ``hate speech'' (e.g., racist content, antisemitism, and so on). The classification is based on models that take web pages as input and produce as output a score that can be used to block certain pages in the advertising ecosystem (or alternatively to produce reports on exposure).  The firm would like to use the system to help protect advertisers, who (despite the best efforts of their advertising agents) sometimes see their ads appearing adjacent to such objectionable content.  The advertisers do not want their brands to be associated with such content, and they definitely do not want to support such content, explicitly or implicitly, with their ad dollars. 

% PANOS: Taking the rarity argument out. Want to focus on estimation of uncertainty/cost.

% Moreover, (fortunately) the prevalence of hate speech in the population of (ad supported) web pages is less that 1 in $10^7$, a base rate orders of magnitude lower than what we see generally in the machine learning literature, even in the literature on learning with class imbalance~\cite{WeissRarity}.

How does this firm assess the strengths and weaknesses of any system
and model?  Unfortunately for applying machine learning, there exists 
no representative, benchmark, ``gold standard'' corpus of hate speech, 
a trait common to many real-world machine learning problems.
Traditional machine learning evaluation and training
methods make an implicit closed-world
assumption~\cite{Reiter77closedworld}. In logical systems, the
closed-world assumption is that the only answers to a query $Q$ are those
that are actually in the database. In the context of predictive
modeling, our closed-world assumption is that our labeled 
data are sufficient to give us a satisfactory estimation of
model performance. Effectively, machine learning methods 
make the assumption that regularities that have no or insufficient
representation in the training data essentially do not exist. 
Unfortunately, such an assumption is dangerously naive 
in applications with limited labeled training
data, small disjuncts~\cite{weiss10disjunct}, and/or possibly unknown
selection biases. In these cases, we face the following problem: 
the model often cannot estimate properly
its own performance on unseen data, and is vulnerable to the problem
of \emph{unknown unknowns}.

We will elaborate throughout the paper, but in a nutshell the problem is the following: in machine learning (ML) research, we assume that the available data are representative and therefore we can use them safely to train and to evaluate our algorithms. Furthermore, and crucially, we assume that the models can estimate properly their own level of performance and report back accurate confidence metrics for different parts of the space. Assuming that we can know how well our algorithms are performing, we can work to improve the models, and/or improve the performance in regions of lower confidence in our predictions, and/or act based on this confidence.  We have faith in our training data, and we focus on what we can know---including \emph{knowing} what we don't know (e.g., regions of uncertainty in our learned model's behavior).

The problem is that, for various reasons, processes that produce training data can completely miss important regions of the space.  Consider our case of identifying hate speech online: The category ``hate speech'' is relatively rare on the Web (fortunately) and extremely varied---a ``disjunctive concept'' in machine learning parlance.  If a certain type of hate speech is not included in the training data, the resultant learned classifier is likely to (i)~classify it as not-hate-speech, and worse, (ii)~do so with high confidence.  Such a case is an \emph{unknown unknown}---the classifier does not ``know'' this sort of hate speech, and it doesn't know that it doesn't know it. So, the unknown unknowns belong to the regions in the space where the model estimates the misclassification cost to be low, while in reality the misclassification cost is high.

Standard ML evaluations may completely miss such issues: cross-validated error rates and area-under-the-curve values for such models may look nearly perfect.  However, the machine learning scientist would be incorrect to report to the application stakeholders that the models are essentially perfect.  It is critical to emphasize that such evaluations only consider the \emph{known unknowns}. However, stakeholders of ML models need to also consider the possible impact of the unknown unknowns. For example, being blindsided by a client who discovers a completely unknown error, or suffering an embarassing PR disaster if the vulnerability of the model is exposed publicly by a third party or competitor.   Finding the unknown unknowns, in a machine learning context, is the focus of our paper.

We present a novel framework for thinking about errors of machine learning models that highlights the unknown unknowns (plenty of work has focused on the known unknowns).  We then present a game-structured system, called \emph{Beat the Machine} that takes advantage of crowdsourcing to help us identify the unknown unknowns.  The system is fully implemented at industrial scale; we discuss several design iterations that each presented hurdles to overcome and led to the current system.  Finally we present experimental results demonstrating that on real problems (including finding hate speech pages), explicitly focusing on finding unknown unknowns (with Beat the Machine) indeed finds pages that were completely missed by the prior system, are systematic errors (rather than just isolated outliers), and are systematically different from the prior training data. 

