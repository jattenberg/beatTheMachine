@inproceedings{Reiter77closedworld,
  author    = {Raymond Reiter},
  title     = {On Closed World Data Bases},
  booktitle = {Logic and Data Bases},
  year      = {1977},
  pages     = {55-76},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@INPROCEEDINGS{Platt99probabilisticoutputs,
    author = {John C. Platt},
    title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
    booktitle = {Advances in Large Margin Classifiers},
    year = {1999},
    pages = {61--74},
    publisher = {MIT Press}
}

@techreport{Perlich2013WP, Author = {Brian Dalessandro and Rod Hook and Claudia Perlich and Foster Provost}, Institution = {New York University}, Number = {CBA-12-02}, Title = {Evaluating and Optimizing Online Advertising: Forget the click, but there are good proxies}, Type = { NYU Working Paper No. CBA-12-02.}, Year = {2012}
}

.  .  (People’s Choice Award Runner-up at the 2012 Empirical Generalizations in Advertising Conference.) NYU Working Paper No. CBA-12-02.

@inproceedings{activelearningtutorial,
 author = {Dasgupta, Sanjoy and Langford, John},
 title = {Tutorial summary: Active learning},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 series = {ICML '09},
 year = {2009},
 isbn = {978-1-60558-516-1},
 location = {Montreal, Quebec, Canada},
 pages = {18:1--18:1},
 articleno = {18},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1553374.1553559},
 doi = {10.1145/1553374.1553559},
 acmid = {1553559},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@article{attenberg:2010inactive,
 Author         = {Josh Attenberg and Foster Provost},
  Title          = {{Inactive Learning? Difficulties Employing Active Learning in Practice}},
  journal   = {SIGKDD Explorations},
  volume    = {12},
  number    = {2},
  year      = {2010},
  pages     = {36--41},
}

@article{herbei:2005,
    abstract = {{The authors study binary classification that allows for a reject option in which case no decision is made. This reject option is to be used for those observations for which the conditional class probabilities are close and as such are hard to classify. The authors generalize existing theory for both plug-in rules and empirical risk minimizers to this setting. La classification \`{a} clause de renvoi Les auteurs s'int\'{e}ressent \`{a} une m\'{e}thode de classification dichotomique permettant de laisser en suspens le classement de certaines observations dont les probabilit\'{e}s conditionnelles d'appartenance \`{a} l'une ou l'autre classe sont si proches qu'un choix est difficile \`{a} faire. Les auteurs \'{e}tudient dans ce contexte les propri\'{e}t\'{e}s des r\`{e}gles de substitution et des r\`{e}gles de classement minimisant le risque empirique.}},
    author = {Herbei, Radu and Wegkamp, Marten H.},
    citeulike-article-id = {8813315},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/cjs.5550340410},
    doi = {10.1002/cjs.5550340410},
    journal = {Can J Statistics},
    number = {4},
    pages = {709--721},
    posted-at = {2011-02-12 16:43:56},
    priority = {2},
    publisher = {Wiley-Blackwell},
    title = {{Classification with reject option}},
    url = {http://dx.doi.org/10.1002/cjs.5550340410},
    volume = {34},
    year = {2006}
}

@article{chow:70,
    abstract = {{The performance of a pattern recognition system is characterized by its error and reject tradeoff. This paper describes an optimum rejection rule and presents a general relation between the error and reject probabilities and some simple properties of the tradeoff in the optimum recognition system. The error rate can be directly evaluated from the reject function. Some practical implications of the results are discussed. Examples in normal distributions and uniform distributions are given.}},
    author = {Chow, C.},
    booktitle = {Information Theory, IEEE Transactions on},
    journal = {IEEE Transactions on Information Theory},
    month = {January},
    number = {1},
    pages = {41--46},
    title = {{On optimum recognition error and reject tradeoff}},

    volume = {16},
    year = {1970}
}

@article{chow:57,
author={Chow, C. K.}, 
journal={Electronic Computers, IRE Transactions on}, title={An optimum character recognition system using decision functions}, 
year={1957}, 
month={dec. }, 
volume={EC-6}, 
number={4}, 
pages={247 -254}, 
}

@inproceedings{lewis94sequential,
    address = {New York, NY, USA},
    author = {Lewis, David D. and Gale, William A.},
    booktitle = {SIGIR '94},
    citeulike-article-id = {1727809},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=188495},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/62437.62470},
    doi = {10.1145/62437.62470},
    isbn = {038719889X},
    keywords = {active\_learning, sequential\_algorithm, text\_classifiers},
    pages = {3--12},
    posted-at = {2009-12-14 16:39:11},
    priority = {2},
    publisher = {Springer-Verlag New York, Inc.},
    title = {A sequential algorithm for training text classifiers},
    url = {http://dx.doi.org/10.1145/62437.62470},
    year = {1994}
}

@inproceedings{domingos1999metacost,
    abstract = {Research in machine learning, statistics and related fields has produced a wide variety of algorithms for classification. However, most of these algorithms assume that all errors have the same cost, which is seldom the case in KDD prob- lems. Individually making each classification learner costsensitive is laborious, and often non-trivial. In this paper we propose a principled method for making an arbitrary classifier cost-sensitive by wrapping a cost-minimizing procedure around it. This procedure, called MetaCost, treats the underlying classifier as a black box, requiring no knowledge of its functioning or change to it. Unlike stratification, MetaCost is applicable to any number of classes and to arbitrary cost matrices. Empirical trials on a large suite of benchmark databases show that MetaCost almost always produces large cost reductions compared to the cost-blind classifier used (C4.5RULES) and to two forms of stratification. Further tests identify the key components of MetaCost and those that can be varied without substantial loss. Experiments on a larger database indicate that MetaCost scales well.},
    author = {Domingos, Pedro},
    booktitle = {KDD '99},

    title = {MetaCost: A General Method for Making Classifiers Cost-Sensitive},
    year = {1999}
}


@inproceedings{elkan:2001cost,
    abstract = {{This paper revisits the problem of optimal learning and decision-making when different misclassification errors incur different penalties. We characterize precisely but intuitively when a cost matrix is reasonable, and we show how to avoid the mistake of defining a cost matrix that is economically incoherent. For the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions...}},
    author = {Elkan, Charles},
    booktitle = {IJCAI},
    citeulike-article-id = {2266473},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.514},
    keywords = {cost-sensitive, decision-making, imbalanced, ml, optimal, rebalancing},
    pages = {973--978},
    posted-at = {2008-01-21 08:52:01},
    priority = {0},
    title = {{The Foundations of Cost-Sensitive Learning}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.514},
    year = {2001}
}

@MISC{Freund99ashort,
    author = {Yoav Freund and Robert E. Schapire},
    title = { A Short Introduction to Boosting},
    year = {1999}
}
@INPROCEEDINGS{bennett:online,
AUTHOR = "Paul N. Bennett and Vitor R. Carvalho",
TITLE = "Online stratified sampling: evaluating classifiers at web-scale.",
booktitle = "CIKM'10",

YEAR = {2010}  }

		
@inproceedings{activerisk2010,
    abstract = {{We address the problem of evaluating the risk of a given model accurately at minimal labeling costs. This problem occurs in situations in which risk estimates cannot be obtained from held-out training data, because the training data are unavailable or do not reflect the desired test distribution. We study active risk estimation processes in which instances are actively selected by a sampling process from a pool of unlabeled test instances and their labels are queried. We derive the sampling distribution that minimizes the estimation error of the active risk estimator when used to select instances from the pool. An analysis of the distribution that governs the estimator leads to confidence intervals. We empirically study conditions under which the active risk estimate is more accurate than a standard risk estimate that draws equally many instances from the test distribution. 1.}},
    author = {{Sawade, Christoph} and {Bickel, Steffen} and {Scheffer, Tobias}},
    citeulike-article-id = {8147558},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.170.2447},
    posted-at = {2010-10-29 17:12:54},
    priority = {2},
    title = {{Active Risk Estimation}},
    booktitle={ICML},
    year={2010},
}
@inproceedings{attprov:kdd2010,
    author = {Attenberg, Josh and Provost, Foster},
        booktitle = {KDD },
        title = {Why label when you can search? Strategies for applying human resources to build classification models under extreme class imbalance},
            year = {2010}
}
@inproceedings{yuanreject,
  author    = {Ming Yuan and
               Marten H. Wegkamp},
  title     = {Classification Methods with Reject Option Based on Convex
               Risk Minimization},
  booktitle   = {Journal of Machine Learning Research},
  volume    = {11},
  year      = {2010},
  pages     = {111-130},
}
@inproceedings{domingosrichardsonKDD2001,
  author    = {Pedro Domingos and
               Matthew Richardson},
  title     = {Mining the network value of customers},
  booktitle = {KDD},
  year      = {2001},

}
@article{AralPNAS09,
    author = {Aral, Sinan and Muchnik, Lev and Sundararajan, Arun},

    journal = {Proceedings of the National Academy of Sciences},
       month = {December},
    number = {51},
    pages = {21544--21549},
   title = {Distinguishing influence-based contagion from homophily-driven diffusion in dynamic networks},
    volume = {106},
    year = {2009}
}
@INPROCEEDINGS{Fumera03f:classification,
    author = {Giorgio Fumera and Ignazio Pillai and Fabio Roli},
    title = {Classification with Reject Option in Text Categorisation Systems},
    booktitle = {In: Proc. 12th International Conference on Image Analysis and Processing. IEEE Computer Society},
    year = {2003},
    pages = {582--587}
}
@article{bartletthinge,
  author    = {Peter L. Bartlett and
               Marten H. Wegkamp},
  title     = {Classification with a Reject Option using a Hinge Loss},
  journal   = {Journal of Machine Learning Research},
  volume    = {9},
  year      = {2008},
  pages     = {1823-1840},
}


@inproceedings{helmboldlabel,
 author = {Helmbold, David and Panizza, Sandra},
 title = {Some label efficient learning results},
 booktitle = {COLT '97: Proceedings of the tenth annual conference on Computational learning theory},
 year = {1997},

 publisher = {ACM},
 }
@article{bianchilabelefficient,
    abstract = {Abstract. We investigate label efficient prediction, a variant of the problem of prediction with expert advice, proposed by Helmbold and Panizza, in which the forecaster does not have access to the outcomes of the sequence to be predicted unless he asks for it, which he can do for a limited number of times. We determine matching upper and lower bounds for the best possible excess error when the number of allowed queries is a constant. We also prove that a query rate of order (ln n)(ln ln n) 2 /n is sufficient for achieving Hannan consistency, a fundamental property in game-theoretic prediction models. Finally, we apply the label efficient framework to pattern classification and prove a label efficient mistake bound for a randomized variant of Littlestone's zero-threshold Winnow algorithm. 1},
    author = {Cesa-bianchi, Nicol\`{o} and Lugosi, G\'{a}bor and Stoltz, Gilles},
    journal = {IEEE Trans. Inform. Theory},
    pages = {77--92},
    title = {Minimizing regret with label efficient prediction},
      volume = {51},
    year = {2005}
}
@ inproceedings{bianchiselective,
    abstract = {A selective sampling algorithm is a learning algorithm for classification that, based on the past observed data, decides whether to ask the label of each new instance to be classified. In this paper, we introduce a general technique for turning linear-threshold classification algorithms from the general additive family into randomized selective sampling algorithms. For the most popular algorithms in this family we derive mistake bounds that hold for individual sequences of examples. These bounds show that our semi-supervised algorithms can achieve, on average, the same accuracy as that of their fully supervised counterparts, but using fewer labels. Our theoretical results are corroborated by a number of experiments on real-world textual data. The outcome of these experiments is essentially predicted by our theoretical results: Our selective sampling algorithms tend to perform as well as the algorithms receiving the true label after each classification, while observing in practice substantially fewer labels.},
    author = {Bianchi, Nicol\`{o} C. and Gentile, Claudio and Zaniboni, Luca},
    booktitle = {J. Mach. Learn. Res.},
    pages = {1205--1230},
    priority = {2},
    title = {Worst-Case Analysis of Selective Sampling for Linear Classification},
    volume = {7},
    year = {2006},
}

@inproceedings{nguyen:clustering,

    author = {Nguyen, Hieu T. and Smeulders, Arnold},
    booktitle = {ICML '04: Proceedings of the twenty-first international conference on Machine learning},
    title = {Active learning using pre-clustering},
    year = {2004}
}

@inproceedings{sculley:onlinespam,

    author = {Sculley, D.},
    booktitle = {Fourth Conf. on Email and AntiSpam},
    title = {Online Active Learning Methods for Fast Label-Efficient Spam Filtering},
    year = {2007}
}

@inproceedings{rattigancollective,
  author    = {Matthew J. Rattigan and
               Marc Maier and
               David Jensen and
               Bin Wu and
               Xin Pei and
               Jianbin Tan and
               Yi Wang},
  title     = {Exploiting Network Structure for Active Inference in Collective
               Classification},
  booktitle = {ICDM Workshops},
  year      = {2007},

}

@inproceedings{bilgiccollective,
 author = {Bilgic, Mustafa and Getoor, Lise},
 title = {Effective label acquisition for collective classification},
 booktitle = {KDD '08},
 year = {2008},
 
 }

@article{fadercounting,
 author = {Fader, Peter S. and Hardie, Bruce G. S. and Lee, Ka Lok},
 title = {"Counting Your Customers" the Easy Way: An Alternative to the Pareto/NBD Model},
 journal = {Marketing Science},
 volume = {24},
 number = {2},
 year = {2005},
 pages = {275--284},
 }

@article{schmittleincounting,
 author = {Schmittlein, David C. and Morrison, Donald G. and Colombo, Richard},
 title = {Counting your customers: who are they and what will they do next?},
 journal = {Manage. Sci.},
 volume = {33},
 number = {1},
 year = {1987},
 issn = {0025-1909},
 pages = {1--24},
 }



@inproceedings{melvillelexical,
 author = {Melville, Prem and Gryc, Wojciech and Lawrence, Richard D.},
 title = {Sentiment analysis of blogs by combining lexical knowledge with text classification},
 booktitle = {KDD '09},
 year = {2009},
 }

@inproceedings{attprovkdd2010,
    author = {Attenberg, Josh and Provost, Foster},
        booktitle = {KDD},
        title = {Why label when you can search? Strategies for applying human resources to build classification models under extreme class imbalance},
            year = {2010}
}
@inproceedings{melvilleADS,
 author = {Melville, Prem and Sindhwani, Vikas},
 title = {Active dual supervision: reducing the cost of annotating examples and features},
 booktitle = {HLT '09},
 year = {2009},
 }

@incollection{weiss10disjunct,
year={2010},
isbn={978-1-4419-1279-4},
booktitle={Data Mining},
volume={8},
series={Annals of Information Systems},
editor={Stahlbock, Robert and Crone, Sven F. and Lessmann, Stefan},
doi={10.1007/978-1-4419-1280-0_9},
title={The Impact of Small Disjuncts on Classifier Learning},
url={http://dx.doi.org/10.1007/978-1-4419-1280-0_9},
publisher={Springer US},
author={Weiss, GaryM.},
pages={193-226},
language={English}
}



@inproceedings{weiss09quant,
    abstract = {In realistic settings the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier. Further complicating this scenario is the fact that labeled data is often scarce and expensive. In this paper we address the problem where the class distribution changes and only unlabeled examples are available from the new distribution. We design and evaluate a number of methods for coping with this problem and compare the performance of these methods. Our quantification-based methods estimate the class distribution of the unlabeled data from the changed distribution and adjust the original classifier accordingly, while our semi-supervised methods build a new classifier using the examples from the new (unlabeled) distribution which are supplemented with predicted class values. We also introduce a hybrid method that utilizes both quantification and semi-supervised learning. All methods are evaluated using accuracy and F-measure on a set of benchmark data sets. Our results demonstrate that our methods yield substantial improvements in accuracy and F-measure.},
    address = {New York, NY, USA},
    author = {Xue, Jack C. and Weiss, Gary M.},
    booktitle = {KDD '09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
    citeulike-article-id = {6557009},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1557117},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1557019.1557117},
    doi = {10.1145/1557019.1557117},
    isbn = {978-1-60558-495-9},
    location = {Paris, France},
    pages = {897--906},
    posted-at = {2010-01-18 16:45:27},
    priority = {2},
    publisher = {ACM},
    title = {Quantification and semi-supervised classification methods for handling changes in class distribution},
    url = {http://dx.doi.org/10.1145/1557019.1557117},
    year = {2009}
}
@inproceedings{donbez07dual,

    author = {Donmez, Pinar and Carbonell, Jaime G. and Bennett, Paul N.},
    booktitle = {ECML '07},
     title = {Dual Strategy Active Learning},
    year = {2007}
}

	
@inproceedings{lomasky2009class,
    abstract = {This paper presents  Active Class Selection  (ACS), a new class of problems for multi-class supervised learning. If one can control the classes from which training data is generated, utilizing feedback during learning to guide the generation of new training data will yield better performance than learning from any  a priori  fixed class distribution. ACS is the process of iteratively selecting class proportions for data generation. In this paper we present several methods for ACS. In an empirical evaluation, we show that for a fixed number of training instances, methods based on increasing class stability outperform methods that seek to maximize class accuracy or that use random sampling. Finally we present results of a deployed system for our motivating application: training an artificial nose to discriminate vapors.},
    address = {Berlin, Heidelberg},
    author = {Lomasky, R. and Brodley, C. E. and Aernecke, M. and Walt, D. and Friedl, M.},
    booktitle = {ECML '07: Proceedings of the 18th European conference on Machine Learning},
    citeulike-article-id = {6463179},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1421731},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/978-3-540-74958-5_63},
    doi = {10.1007/978-3-540-74958-5_63},
    isbn = {978-3-540-74957-8},
    keywords = {active\_class\_selection, active\_learning},
    location = {Warsaw, Poland},
    pages = {640--647},
    posted-at = {2009-12-31 19:06:26},
    priority = {5},
    publisher = {Springer-Verlag},
    title = {Active Class Selection},
    url = {http://dx.doi.org/10.1007/978-3-540-74958-5_63},
    year = {2007}
}

	
@InProceedings{zhu2007imbalance,
  author    = {Zhu, Jingbo  and  Hovy, Eduard},
  title     = {Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem},
  booktitle = { EMNLP-CoNLL 2007},

  year = {2007},
  
}



@inproceedings{bloodgood2009imbalance,
    abstract = {Actively sampled data can have very different characteristics than passively sampled data. Therefore, it's promising to investigate using different inference procedures during AL than are used during passive learning (PL). This general idea is explored in detail for the focused case of AL with cost-weighted SVMs for imbalanced data, a situation that arises for many HLT tasks. The key idea behind the proposed InitPA method for addressing imbalance is to base cost models during AL on an estimate of overall corpus imbalance computed via a small unbiased sample rather than the imbalance in the labeled training data, which is the leading method used during PL.},
    author = {Bloodgood, Michael and Shanker, K. Vijay},
    booktitle = {NAACL '09},

    title = {Taking into account the differences between actively and passively acquired data: the case of active learning with support vector machines for imbalanced datasets},
    year = {2009}
}

	

@inproceedings{tomanek2009imbalance,
    abstract = {In lots of natural language processing tasks, the classes to be dealt with often occur heavily imbalanced in the underlying data set and classifiers trained on such skewed data tend to exhibit poor performance for low-frequency classes. We introduce and compare different approaches to reduce class imbalance by design within the context of active learning (AL). Our goal is to compile more balanced data sets up front during annotation time when AL is used as a strategy to acquire training material. We situate our approach in the context of named entity recognition. Our experiments reveal that we can indeed reduce class imbalance and increase the performance of classifiers on minority classes while preserving a good overall performance in terms of macro F-score.},
    author = {Tomanek, Katrin and Hahn, Udo},
    booktitle = {K-CAP '09},

    title = {Reducing class imbalance during active learning for named entity annotation},

    year = {2009}
}

	

@article{baram2004mixing,
    abstract = {This work is concerned with the question of how to combine online an ensemble of active learners so as to expedite the learning progress in pool-based active learning. We develop an active-learning master algorithm, based on a known competitive algorithm for the multiarmed bandit problem. A major challenge in successfully choosing top performing active learners online is to reliably estimate their progress during the learning session. To this end we propose a simple maximum entropy...},
    author = {Baram, Yoram and El-Yaniv, Ran and Luz, Kobi},
    citeulike-article-id = {1730465},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.4058},
    journal = {Journal of Machine Learning Research},
    keywords = {active\_learning, bandit},
    pages = {255--291},
    posted-at = {2009-12-14 19:26:12},
    priority = {5},
    title = {Online Choice of Active Learning Algorithms},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.4058},
    volume = {5},
    year = {2004}
}

	
	
@article{LiuUndersampling2009,
    abstract = {<para> Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. <formula formulatype="inline"><tex Notation="TeX">\${tt EasyEnsemble}\$</tex> </formula> samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. <formula formulatype="inline"><tex Notation="TeX">\${tt BalanceCascade}\$</tex></formula> trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the ROC Curve, F-measure, and G-mean values than many existing class-imbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods. </para>},
    author = {Liu, X. Y. and Wu, J. and Zhou, Z. H.},
    booktitle = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on},
       title = {Exploratory Undersampling for Class-Imbalance Learning},
	  year = {2009}
}

	
@article{chawlaSMOTE2002,
    abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \&amp;quot;normal \&amp;quot; examples with only a small percentage of \&amp;quot;abnormal \&amp;quot; or \&amp;quot;interesting \&amp;quot; examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy. 1.},
    author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Kegelmeyer, Philip W.},
      journal = {J. Artif. Int. Res. },
    pages = {321--357},
    title = {SMOTE: Synthetic Minority Over-sampling Technique},

    volume = {16},
    year = {2002}
}

	

@inproceedings{ErtekinBorder2007,
    abstract = {This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly less number of observations of the target concept. Various real-world classification tasks, such as medical diagnosis, text categorization and fraud detection suffer from this phenomenon. The standard machine learning algorithms yield better prediction performance with balanced datasets. In this paper, we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes. We also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire dataset. The proposed method yields an efficient querying system and allows active learning to be applied to very large datasets. Our experimental results show that with an early stopping criteria, active learning achieves a fast solution with competitive prediction performance in imbalanced data classification.},
    address = {New York, NY, USA},
    author = {Ertekin, Seyda and Huang, Jian and Bottou, Leon and Giles, Lee},
    booktitle = {CIKM '07},
      title = {Learning on the border: active learning in imbalanced data classification},

    year = {2007}
}

	

@inproceedings{donmez2008psd,
    author = {Donmez, P. and Carbonell, J.},
    booktitle = {Proc. 10 ths International Symposium on Artificial Intelligence and Mathematics},
    title = {{Paired Sampling in Density-Sensitive Active Learning}},
    year = {2008}
}

	
@inproceedings{zhuDensity2008,

    author = {Zhu, Jingbo and Wang, Huizhen and Yao, Tianshun and Tsou, Benjamin K.},
    booktitle = {COLING '08},
     title = {Active learning with sampling by uncertainty and density for word sense disambiguation and text classification},
    year = {2008}
}

	

@techreport{settles.tr09, Author = {Burr Settles}, Institution = {University of Wisconsin--Madison}, Number = {1648}, Title = {Active Learning Literature Survey}, Type = {Computer Sciences Technical Report}, Year = {2009}
}


@inproceedings{ma2009beyond,
    abstract = {Malicious Web sites are a cornerstone of Internet criminal activities. As a result, there has been broad interest in developing systems to prevent the end user from visiting such sites. In this paper, we describe an approach to this problem based on automated URL classification, using statistical methods to discover the tell-tale lexical and host-based properties of malicious Web site URLs. These methods are able to learn highly predictive models by extracting and automatically analyzing tens of thousands of features potentially indicative of suspicious URLs. The resulting classifiers obtain 95-99\% accuracy, detecting large numbers of malicious Web sites from their URLs, with only modest false positives.},
    author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
    booktitle = {KDD '09},
    title = {Beyond blacklists: learning to detect malicious web sites from suspicious URLs},
    year = {2009}
}


@inproceedings{lomasky2007class,
    abstract = {This paper presents  Active Class Selection  (ACS), a new class of problems for multi-class supervised learning. If one can control the classes from which training data is generated, utilizing feedback during learning to guide the generation of new training data will yield better performance than learning from any  a priori  fixed class distribution. ACS is the process of iteratively selecting class proportions for data generation. In this paper we present several methods for ACS. In an empirical evaluation, we show that for a fixed number of training instances, methods based on increasing class stability outperform methods that seek to maximize class accuracy or that use random sampling. Finally we present results of a deployed system for our motivating application: training an artificial nose to discriminate vapors.},
    address = {Berlin, Heidelberg},
    author = {Lomasky, R. and Brodley, C. E. and Aernecke, M. and Walt, D. and Friedl, M.},
    booktitle = {ECML '07: Proceedings of the 18th European conference on Machine Learning},
    citeulike-article-id = {6463179},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1421731},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/978-3-540-74958-5_63},
    doi = {10.1007/978-3-540-74958-5_63},
    isbn = {978-3-540-74957-8},
    keywords = {active\_class\_selection, active\_learning},
    location = {Warsaw, Poland},
    pages = {640--647},
    posted-at = {2009-12-31 19:06:26},
    priority = {2},
    publisher = {Springer-Verlag},
    title = {Active Class Selection},
    url = {http://dx.doi.org/10.1007/978-3-540-74958-5_63},
    year = {2007}
}


@misc{fawcett03roc,
    abstract = {Receiver Operating Characteristics (ROC) graphs are a useful technique for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been increasingly adopted in the machine learning and data mining research communities. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. This article serves both as a tutorial introduction to ROC graphs and...},
    author = {Fawcett, T.},
    citeulike-article-id = {1113649},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.6444},
    posted-at = {2009-12-31 18:47:26},
    priority = {2},
    title = {ROC Graphs: Notes and Practical Considerations for Data Mining Researchers},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.6444},
    year = {2003}
}
@article{Saar:2004,
    abstract = {In many cost-sensitive environments class probability estimates are used by decision makers to evaluate the expected utility from a set of alternatives. Supervised learning can be used to build class probability estimates; however, it often is very costly to obtain training data with class labels. Active learning acquires data incrementally, at each phase identifying especially useful additional data for labeling, and can be used to economize on examples needed for learning. We outline the critical features of an active learner and present a sampling-based active learning method for estimating class probabilities and class-based rankings. BOOTSTRAP-LV identifies particularly informative new data for learning based on the variance in probability estimates, and uses weighted sampling to account for a potential example's informative value for the rest of the input space. We show empirically that the method reduces the number of data items that must be obtained and labeled, across a wide variety of domains. We investigate the contribution of the components of the algorithm and show that each provides valuable information to help identify informative examples. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV uses fewer examples to exhibit a certain estimation accuracy and provide insights to the behavior of the algorithms. Finally, we experiment with another new active sampling algorithm drawing from both UNCERTAINTY SAMPLING and BOOTSTRAP-LV and show that it is significantly more competitive with BOOTSTRAP-LV compared to UNCERTAINTY SAMPLING. The analysis suggests more general implications for improving existing active sampling algorithms for classification.},
    author = {Saar-Tsechansky, Maytal and Provost, Foster},

    journal = {Machine Learning},
    keywords = {active\_learning, critical\_features, probability\_estimation, sampling\_algorithms},
    number = {2},
    pages = {153--178},
    posted-at = {2009-12-19 18:25:34},
    priority = {3},
    publisher = {Springer},
    title = {Active Sampling for Class Probability Estimation and Ranking},
    volume = {54},
    year = {2004}
}
@INPROCEEDINGS{Saar-tsechansky01activelearning,
    author = {Maytal Saar-tsechansky and Foster Provost},
    title = {Active Learning for Class Probability Estimation and Ranking},
    booktitle = {In Proc of the Seventeenth Int Joint Fonf on Artificial Intelligence (IJCAI-2001},
    year = {2001},
    pages = {911--920}
}



@inproceedings{shengKDD2008,
    abstract = {This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.},

    author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
    booktitle = {KDD '08},
   
    title = {Get another label? improving data quality and data mining using multiple, noisy labelers},

    year = {2008}
}
@article{weinbergerICML2009,
    abstract = {Empirical evidence suggests that hashing is an effective strategy for
dimensionality reduction and practical nonparametric estimation. In this paper
we provide exponential tail bounds for feature hashing and show that the
interaction between random subspaces is negligible with high probability. We
demonstrate the feasibility of this approach with experimental results for a
new use case -- multitask learning with hundreds of thousands of tasks.},
    archivePrefix = {arXiv},
    author = {Weinberger, Kilian and Dasgupta, Anirban and Attenberg, Josh and Langford, John and Smola, Alex},
    citeulike-article-id = {5035610},
    citeulike-linkout-0 = {http://arxiv.org/abs/0902.2206},
    citeulike-linkout-1 = {http://arxiv.org/pdf/0902.2206},
    day = {21},
    eprint = {0902.2206},
    keywords = {feature\_hashing},
    month = {May},
    posted-at = {2009-12-31 17:24:49},
    priority = {2},
    title = {Feature Hashing for Large Scale Multitask Learning},
    url = {http://arxiv.org/abs/0902.2206},
    year = {2009}
}
@article{WeissProvost2003,
    abstract = {For large, real-world inductive learning problems, the number of training examples often must be limited due to the costs associated with procuring, preparing, and storing the training examples and/or the computational costs associated with learning from them. In such circumstances, one question of practical importance is: if only  n  training examples can be selected, in what proportion should the classes be represented? In this article we help to answer this question by analyzing, for a fixed training-set size, the relationship between the class distribution of the training data and the performance of classification trees induced from these data. We study twenty-six data sets and, for each, determine the best class distribution for learning. The naturally occurring class distribution is shown to generally perform well when classifier performance is evaluated using undifferentiated error rate (0/1 loss). However, when the area under the ROC curve is used to evaluate classifier performance, a balanced distribution is shown to perform well. Since neither of these choices for class distribution always generates the best-performing classifier, we introduce a "budget-sensitive" progressive sampling algorithm for selecting training examples based on the class associated with each example. An empirical analysis of this algorithm shows that the class distribution of the resulting training set yields classifiers with good (nearly-optimal) classification performance.},
    author = {Weiss, Gary M. and Provost, Foster},

    journal = {J. Artif. Int. Res.},

    pages = {315--354},

    publisher = {AI Access Foundation},
    title = {Learning when training data are costly: the effect of class distribution on tree induction},
    volume = {19},
    year = {2003}
}


@misc{WeissProvost2001,
    abstract = {In this article we analyze the effect of class distribution on classifier learning. We begin by describing

the different ways in which class distribution affects learning and how it affects the

evaluation of learned classifiers. We then present the results of two comprehensive experimental

studies. The first study compares the performance of classifiers generated from unbalanced data

sets with the performance of classifiers generated from balanced versions of the same data sets.

This...},
    author = {Weiss, G. and Provost, F.},
    citeulike-article-id = {1284148},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.9570},
    comment = {this article deals with the influence of class distribution on the performance on classifier performance. The motivating idea here is that biasing the prior distribution during training may be beneficial during testing time, provided that these class probabilities can be corrected (using the same method as elkan 2001). Such biases would include over-sampling the minority class, and training on a uniform distribution. Note that all experiments are performed using C4.5, and therefore are primarily focused on managing the frequentist probability estimates at the regions of parameter space defined by the tree's leaves. The first half of this paper is mainly empirical to test common assumptions.

the first experiment trains and tests on instances drawn from sets with a variety of skews in class distributions. Importantly, in 22/26 sets, most of the errors come from the minority class: true minority instances are misclassified more frequently (26/26), and majority instances are falsely identified as minority more frequently then the vice versa. 

the proportion of the minority class in training is varied to find the optimal value. When the goal is to maximize accuracy, the natural distribution seems appropriate, while when maximizing AUC, a more uniform distribution seems more appropriate, sometimes even favoring the minority class. The authors note, however that different training distributions lead to varying performances in different regions of ROC space; a large bias to the minority leads to better performance in the high FP region of the ROC space, and vice versa. 

To emphasize the importance of the class distribution in training, the overall training size AND the distribution are varied. it is shown that choosing the best training distribution can offer better test performance than an inappropriate distribution with 2x or 4x the training data. This motivates the need to carefully select class when building a training set. 


the proposed selection aglorithm takes n (max \# selections) cmin ( the min proportion of the minority class) and (the geometric factor for step size ). a best ratio is evaluated empriacally by incrementally trained classifiers, and beam search is used to test near-by ratios. successive iterations are sampled on the current best estimated ratio. size is geometrically increasing as a fn of mu, the num chosen by the bottom and top of the beam. },
    keywords = {class\_distribution, class\_selection},
    posted-at = {2009-12-15 17:20:06},
    priority = {0},
    title = {The effect of class distribution on classifier learning},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.9570},
    year = {2001}
}


@misc{dmoz,
  howpublished = "\url{http://www.dmoz.org/}"
}
@misc{AMT,
  howpublished = "\url{https://www.mturk.com/mturk/welcome}"
}


@misc{federovOptimal1972 ,
    author = "V. V. Federov",
    year = "1972",
    title = "Theory of optimal experiments",
publisher = {Academic Press},
}


@misc{uci ,
    author = "A. Asuncion and D.J. Newman",
    year = "2007",
    title = "{UCI} Machine Learning Repository",
    url = "http://www.ics.uci.edu/$\sim$mlearn/{MLR}epository.html",
    institution = "University of California, Irvine, School of Information and Computer Sciences" }
@misc{mechanicalturk,
  howpublished = "\url{https://www.mturk.com/}"
}
@article{cohn1992:activeLearning,
    abstract = {Active learning differs from  ï¿½ learning from examplesï¿½ in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples.

In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers  ï¿½ useful.ï¿½ We test our implementation, called an SG-network, on three domains and observe significant improvement in generalization.},
    address = {Hingham, MA, USA},
    author = {Cohn, David and Atlas, Les and Ladner, Richard},
    citeulike-article-id = {1727805},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=189489},
    citeulike-linkout-1 = {http://dx.doi.org/10.1023/A:1022673506211},
    doi = {10.1023/A:1022673506211},
    issn = {0885-6125},
    journal = {Mach. Learn.},
    keywords = {active\_learning, ai, pbd},
    month = {May},
    number = {2},
    pages = {201--221},
    posted-at = {2008-05-23 00:20:43},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Improving Generalization with Active Learning},
    url = {http://dx.doi.org/10.1023/A:1022673506211},
    volume = {15},
    year = {1994}
}

	
@MISC{Seung92queryby,
    author = {H. S. Seung and M. Opper and H. Sompolinsky},
    title = {Query by Committee},
    year = {1992}
}
@article{tong02svm,
    abstract = {Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using \&lt;em\&gt;pool-based active learning\&lt;/em\&gt;. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a \&lt;em\&gt;version space\&lt;/em\&gt;. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.},
    address = {Cambridge, MA, USA},
    author = {Tong, Simon and Koller, Daphne},
    citeulike-article-id = {716139},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=944793},
    citeulike-linkout-1 = {http://dx.doi.org/10.1023/A:1009715923555},
    doi = {10.1023/A:1009715923555},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    keywords = {active, learning, margin, svm},
    pages = {45--66},
    posted-at = {2009-12-14 15:06:35},
    priority = {2},
    publisher = {JMLR.org},
    title = {Support vector machine active learning with applications to text classification},
    url = {http://dx.doi.org/10.1023/A:1009715923555},
    volume = {2},
    year = {2002}
}

@MISC{SettlesActiveLearning,
    author = {Burr Settles},
    title = {Active Learning Literature Survey},
institute = {University of Wisconsinâ€“Madison} ,
    year = {2010},
}

@conference{raykar2009supervised,
  title={{Supervised Learning from Multiple Experts: Whom to trust when everyone lies a bit}},
  author={Raykar, V.C. and Yu, S. and Zhao, L.H. and Jerebko, A. and Florin, C. and Valadez, G.H. and Bogoni, L. and Moy, L.},
  booktitle={ICML},
  pages={889--896},
  year={2009},
  organization={ACM}
}

@conference{forman2006quantifying,
  title={{Quantifying trends accurately despite classifier error and class imbalance}},
  author={Forman, G.},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={157--166},
  isbn={1595933395},
  year={2006},
  organization={ACM}
}



@techreport{winston1970learning,
 author = {Winston, Patrick H.},
 title = {Learning Structural Descriptions From Examples},
 year = {1970},
 number = {AITR-231},
 url = {\url{http://hdl.handle.net/1721.1/6884}},
 institution = {Massachusetts Institute of Technology},
 address = {Cambridge, MA, USA},
} 



@article{vanlehn1998analogy,
  title={{Analogy events: How examples are used during problem solving}},
  author={VanLehn, K.},
  journal={Cognitive Science},
  volume={22},
  number={3},
  pages={347--388},
  issn={0364-0213},
  year={1998},
  publisher={Elsevier}
}


