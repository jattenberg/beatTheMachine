\section{Impact in Industrial Deployments}

The Beat the Machine design has directly changed the way several companies view and practice the evaluation of predictive systems.  In our example domain for this paper, one medium-sized company that does massive-scale webpage classification in the online advertising space has decided to move beyond its traditional system evaluation methods.  Traditionally, before deployment models have been evaluated using cross-validation, expert model examination, and stratified case examination.  Once a prediction system was deployed in practice, evaluation was based on continual stratified examination from the production classification stream.  Let's call this practice ``passive testing.'' 
According to the firm's founding data scientist, this work convinced this company that Beat the Machine and other ``active testing'' practices are vital to understand their predictive models' performance and alert stakeholders about areas of concern.  The most convincing impact is that this firm has invested in the industrial development of Beat the Machine, and is pursuing its use across classification tasks (not just objectionable content). 

Beat the Machine also has directly influenced the workflow design of a large firm that runs one of the most popular online labor marketplaces. Now, any automatic algorithmic system that gets deployed is tested by asking users to find cases that will break the system. For example, to test a job classification engine, users are asked to submit job descriptions that are legitimately and unambiguously classified into one category, but which the automatic system will classify into another. One of the interesting side-effects of this practice is that it catches early shifts in the content of the typical job posting.  So, when a new type of task starts emerging in the market (and it cannot be classified properly by the current automated engine) the BTM system is likely to catch this trend early, before it becomes a major issue of user dissatisfaction.

Finally, the BTM system has been deployed as part of an image tagging service for a third company.  According to one of the founders, before BTM, automatic systems together with humans were used to tag images with keywords. Under the new BMT-style design, there is an extra phase where humans are looking at an image to ``challege'' existing tags, with the goal that the newly provided tag will be better and more relevant than the currently assigned one.  This design allows for higher quality keywords to be assigned to the images, avoiding cases where only a set of generic, uninteresting keywords are assigned to an image (either by algorithms or by humans).

In general, the main practical impact of the BTM system is the new approach for testing and debugging automatic machine learning models. The technique of rewarding users for locating vulnerabilities and bugs is common in security, and conceptually similar to BMT. However, there is a difference: When dealing with statistical models, merely locating an incorrect classification decision is hardly an event worth rewarding, or even recording. Our BTM system is designed to reward cases where the model exhibits a systematic failure, about which we are not aware until it happens.

\section{Conclusion and Future Work}

% \panos{Perhaps we should also point that the system is 
% implemented at AdSafe and also point to an open source 
% version of the system, available at buildaclassifier.com. 
% This may satisfy the infusion requirement}

We presented the problem of ``unknown unknowns'' in the setting of predictive modeling and explored the design of the \emph{Beat the Machine} process for directly integrating humans into testing automatic decision models for severe vulnerabilities. Our results suggest that BTM is especially good in identifying cases where the model fails, while being confident that it is correct.   Several companies have implemented systems based on the ideas of Beat the Machine, after having read initial reports of this research.

% The BTM process, through its game-like structure and probing nature, encourages the discovery of unknown problems in the model. The fact that humans can easily find challenging cases for the automatic models, when being themselves confronted with this challenge, also indicates that human expertise and curiosity can improve even very accurate automatic models---and should be integrated more broadly into the evaluation of model-based systems. 

We believe that machine learning research should devote more study to this sort of system.  Presumably, even though we have gone through several design iterations, there is a lot to learn about how to design such systems to work well.  As we discussed in the deployment section, in addition to using BTM proper, companies already have been using the ideas in ways slightly different from the exact design we've presented. 
Furture, it is naturally interesting to ask how to best use knowledge of such vulnerabilities to improve the automatic decisions models.  To our knowledge, no work has yet studied this.  Our preliminary experiments indicate that building predictive models in the BTM setting is a very complicated problem. For example, oversampling cases where a model makes big mistakes can be catastrophic for  learning (think simply about oversampling outliers in a linear regression). On the other hand, techniques like boosting~\cite{Freund99ashort} have gotten tremendous advantage by overweighting cases where the current model is incorrect. The potential benefit of being able to simultaneously explore a model's unknowns and offer robust model improvement would be an exciting advance.






% Vulnerability testing is common in areas of computer security, where ``white hat'' hackers with the appropriate expertise try to expose vulnerabilities in the security infrastructure of a firm. In our setting, we see that even lay users can easily find unknown holes in automatic decision models that test very well in ``standard'' tests, and show high classification performance when measured with the traditional, usual metrics (accuracy, AUC, etc).  Thus, builders of automatic decision models should take extra care when using these
% traditional metrics for evaluations.

% In our live deployment, untrained humans, with the appropriate incentives, were able to ``beat the machine'' seemingly easily, and discover a large number of vulnerabilities. This is, of course, useful by itself: the ``unknown unknowns'' become ``known unknowns'' and we can prepare to deal with these cases. But the key question for future research is also: how can we best incorporate such knowledge so that both ``unknown unknowns'' and ``known unknowns'' become ``known knowns.''

% \section*{Acknowledgements}

% The authors thank George A. Kellner and NEC for faculty fellowships,
% and AdSafe Media for expertise, support, and data.  The models used in
% this paper are not necessarily models used in production by any
% company. 


% [Add ambiguity in target variable to Limitations]

% ***To add to current/future work below:

%- How can we actually improve models with these unknown unknown cases?
%  Just plunking them into a training set yields mixed results.  This
%  may be because the training gets skewed, and we need to have a
%  specially designed training system.  Or it may be because we do not
%  really know how to evaluate the ``improved'' system.  What should be
%  the composition of the test set exactly?

%- In KDD-2010 we introduced guided learning, and showed that it can be
%  very useful for quickly building models in domains such as this.  We
%  are in the process of performing a similar comparison of BTM to GL.
%  Notably, GL also is not focused at all on the hard-to-envision
%  cases; in contrast, the incentive system there is to give easy to
%  find cases.

% For discussion:

% - relate to scenario planning
% - relate to white-hat hackers
