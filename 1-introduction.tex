\section{Introduction}
\label{sec:intro}


Many businesses and government organizations make decisions based on
estimations made by explicit or implicit models of the world.  Being
based on models, the decisions are not perfect.  Understanding the
imperfections of the models is important (i)~in order to improve the
models (where possible), (ii)~in order to prepare to deal with the
decision-making errors, and (iii)~in some cases in order to properly
hedge the risks.  However, a crucial challenge is that, for 
complicated decision-making scenarios, we often do not know where 
models of the world are imperfect and/or how the models' imperfections
will impinge on decision making. \emph{ We don't know what we don't know.}

We see the results of such failures of omniscience in grand
catastrophes, from terrorist attacks to unexpected nuclear disasters,
in mid-range failures, like cybersecurity breaches, and in failures of
operational models, such as predictive models for credit scoring,
fraud detection, document classification, etc.

Unknown unknowns are related to the classic contrast between reasoning
systems that make open- and closed-world
assumptions~\cite{Reiter77closedworld}: are the only answers to a query Q those
that are actually in the database?  In the context of predictive
modeling, in applications with limited labeled training data, small
disjuncts\cite{weiss10disjunct}, and possibly unknown selection biases, are
we willing to make the assumption that regularities that have no or
insufficient representation in the training data essentially do not
exist?

% In the context of data mining, one can
% think of learning methods as implicitly or explicitly making a
% closed-world assumption: 
% Whether
% learning systems truly make a closed-world assumption may be debatable
% under strict assumptions of learning theory and decision theory, where
% training data are sampled from the same population to which the model
% will be applied, and we know the costs of making errors.  However, in
% the real world we often learn and apply models anyway in the face of
% various sampling biases, often that we do not even know about.  Furthermore, 
% as we will describe, we often have 

In this paper we introduce and analyze a crowdsourcing system designed
to help uncover the ``unknown unknowns'' for predictive models.  The
system is designed to apply to settings where assessing the
performance of predictive models is particularly challenging.  Later we
will describe in detail the critical aspects of such settings, but
first let us introduce a motivating example to make the discussion
concrete.

Consider the following task: a firm has built a system for identifying
web pages that contain instances of ``hate speech'' (e.g., racist
content, antisemitism, and so on), based on a model that takes web
pages as input and produces as output a ``hate score.''  The firm
would like to use this system to help protect advertisers, who
(despite the best efforts of their advertising agents) sometimes see
their ads appearing adjacent to such objectionable content.  The
advertisers do not want their brands to be associated with such
content, and they definitely do not want to support such content,
explicitly or implicitly, with their ad dollars.  

How does this firm assess the strengths and weaknesses of its system
and model?  This scenario comprises a constellation of factors that
are not uncommon in organizational decision making, but are quite
problematic for conducting the assessment---particularly because of
the problem of unknown unknowns.  Specifically, this paper considers
applications where:

\begin{itemize}
\itemsep=0.0in
\item Every decision-making case can be represented by a description
  and a target.  We have a (predictive) model that can give us an estimate or
  score for the target for any case.  For this paper, we assume for
  simplicity that the target is binary, and that the truth would not
  be in dispute if known.\footnote{For our example, the
  description of the case would be the web page (its words, links,
  images, metadata, etc.).  The target would be whether or not it
  contains hate speech.}

\item We want to understand the inaccuracies of the
  model---specifically, the errors that it makes, and especially
  whether there are systematic patterns in the errors.  For example,
  is there a particular sort of hate speech that the model builders
  did not consider, and therefore the model misses it?

\item The process that is producing the data does not (necessarily)
  \textit{reveal} the target for free.  In our example, if we
  misclassify a hate speech page as being OK, we may never know.
  (Indeed, we usually never know.)  This is in contrast to
  \textit{self-revealing} processes; for example, in the case of credit-card
  fraud detection, we will eventually will be informed by the customer
  that there is fraud on her account.  For targeted marketing, we
  often eventually know whether the consumer responded to an offer or
  not.

\item Finally, there are important classes or subclasses of cases that
  are very rare, but nevertheless very important.  The rarity often is
  the very reason these cases were overlooked in the design of the
  system.  In our example, hate speech on the web itself is quite
  rare (thankfully).  Within hate speech, different subclasses are
  more or less rare.  Expressions of racial hatred are more common
  than expressions of hatred toward dwarves or data miners (both real cases).

\end{itemize}

These problem characteristics combine to make it extremely difficult to
discover system/model imperfections.  Just running the system, in
vitro or in vivo, does not uncover problems; as we do not observe
the true value of the target, we cannot compare the target to the model's
estimation or to the system's decision.

We \textit{can} invest in acquiring data to help us uncover
inaccuracies.  For example, we can task humans to score random or
selected subsets of cases.  Unfortunately, this has two major
drawbacks.  First, due to the rarity of the class of interest (e.g.,
hate speech) it can be very costly to find very few positive examples,
especially via random sampling of pages.  For example, hate speech
represents far less that $0.1\%$ of the population of web pages, with unusual or distinct forms of hate speech being far rarer still. Thus we would
have to invest in labeling more than 1000 web pages just to get one hate speech
example, and as has been pointed out
recently, often you need more than one label per page to get
high-quality labeling~\cite{shengKDD2008,raykar2009supervised}.


% whatever this means for model
% performance~\cite{forman2006quantifying}


In practice, we often turn to particular heuristics to identify
cases that can help to find the errors of our model.  There has been a
large amount of work studying ``active learning'' which attempts to
find particularly informative examples~\cite{SettlesActiveLearning}.
A large number of these strategies (uncertainty sampling, sampling
near the separating hyperplane, query-by-committee, query-by-bagging,
and others) essentially do the same thing: they choose the cases where
the model is least certain, and invest in human labels for these.
This strategy makes sense, as this is where we would think to find
errors.  Additionally, there has been a long history of understanding that
``near misses'' are the cases to use to best improve a model, both for
machine learning~\cite{winston1970learning} and for human
learning~\cite{vanlehn1998analogy}.

Unfortunately, although helpful in understanding and improving
modeling, for finding unknown unknowns, 
these strategies look exactly where we don't want to look.
These strategies explicitly deal with the ``known unknowns.''  The
model is uncertain about these examples---we ``know'' that we don't
know the answer for them (i.e., we have low confidence in the model's
output).  These strategies explicitly eschew, or in some cases
probabilistically downweight, the cases that we are
certain about, thereby \textit{reducing} the chance that we are going
to find the unknown unknowns.

With that substantial preamble, we can now state succinctly the goal
and contributions of this paper.  First, we describe the problem more
formally, including relationships to prior work.  We next discuss
changes to how we need to view the evaluation of classifiers, if we
want to move from a closed-world view of a predictive modeling problem
to an open-world view.  Then we introduce a technique and system to use
human workers to help find the \emph{unknown unknowns}.  Our
BeatTheMachine (BTM) system combines a game-like setup with incentives
designed to elicit cases where the model is confident and wrong.
Specifically, BTM rewards workers that discover cases that cause the
system to fail. The reward increases with the magnitude of the
failure. This setting makes the system to behave like a game,
encouraging steady, accurate participation in the tasks. We describe
our first experiences by the live deployment of this system, in a
setting for identifying web pages with offensive content on the
Internet. We show that this BTM setting discovers cases that are
inherently different than the errors identified by a random sampling
process. In fact, the two types of errors are very different. The BTM
process identifies ``big misses'' and potential catastrophic failures,
while traditional model-based example selection identifies ``near
misses'' that are more appropriate for fine-tuning the system.  The
evidence shows that BTM does not just find individual ``oddball''
outlier cases, but it finds systematic big errors.  In a sense, the
BTM process indeed gives us the opportunity to learn our ``unknown
unknowns'' and warn us about the failures that our current automatic
model cannot (yet) identify by itself.


