\section{Impact in Industrial Deployments}
\label{sec:impact}

The Beat the Machine design has directly changed the way several companies view and practice the evaluation of predictive systems.  In our example domain for this paper, Integral Ad Science,\footnote{The website of Integral Ad Science is available at \url{http://integralads.com/}. Provost and Attenberbg were the firm's founding data scientists, Ipeirotis serves as the a scientific advisor.} one medium-sized company that does massive-scale webpage classification in the online advertising space has decided to move beyond its traditional system evaluation methods.  Traditionally, before deployment models have been evaluated using cross-validation, expert model examination, and stratified case examination.  Once a prediction system was deployed in practice, evaluation was based on continual stratified examination from the production classification stream.  Let's call this practice ``passive testing.'' 
According to the firm's founding data science team, this work convinced Integral Ad Science that Beat the Machine and other ``active testing'' practices are vital to understand their predictive models' performance and alert stakeholders about areas of concern.  The most convincing impact is that Integral Ad Science has invested in the industrial development of Beat the Machine, and is pursuing its use across classification tasks (not just objectionable content). 

Beat the Machine also has influenced the workflow design of oDesk\footnote{The website of oDesk is available at \url{http://www.odesk.com/}. Ipeirotis served as the Academic-in-Residence there and helped design the job classification system.} that runs one of the most popular online labor marketplaces: An automatic algorithmic system before being deployed is tested by asking users to find cases that will break the system. For example, to test a job classification engine, contractors are hired and asked to submit job descriptions that are legitimately and unambiguously classified into one category, but which the automatic system will classify into another. One of the interesting side-effects of this practice is that it catches early shifts in the content of the typical job posting.  So, when a new type of task starts emerging in the market (and it cannot be classified properly by the current automated engine) the BTM system is likely to catch this trend early, before it becomes a major issue of user dissatisfaction.

Finally, the a BTM-like system has been deployed as part of an image tagging service for Tagasauris.\footnote{The website of Tagasauris is available at \url{http://www.tagasauris.com/}. Attenberg is the lead data scientist at Tagasauris, Ipeirotis is a co-founder, and Provost is a member of the advisory board.} Before BTM, automatic systems together with humans were used to tag images with keywords. Under the new BMT-style design, there is an extra phase where humans are looking at an image to ``challege'' existing tags, with the goal that the newly provided tag will be better and more relevant than the currently assigned one.  This design allows for higher quality keywords to be assigned to the images, avoiding cases where only a set of generic, uninteresting keywords are assigned to an image (either by algorithms or by humans).

In general, the main practical impact of the BTM system is the new approach for testing and debugging automatic machine learning models. The technique of rewarding users for locating vulnerabilities and bugs is common in security, and conceptually similar to BMT. However, there is a difference: When dealing with statistical models, merely locating an incorrect classification decision is hardly an event worth rewarding, or even recording. Our BTM system is designed to reward cases where the model exhibits a systematic failure, about which we are not aware until it happens.