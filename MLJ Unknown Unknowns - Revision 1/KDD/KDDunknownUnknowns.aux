\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:intro}{{1}{1}}
\citation{Reiter77closedworld}
\citation{weiss10disjunct}
\citation{shengKDD2008,raykar2009supervised}
\citation{SettlesActiveLearning}
\citation{winston1970learning}
\citation{vanlehn1998analogy}
\citation{elkan:2001cost}
\citation{domingos1999metacost, Platt99probabilisticoutputs}
\citation{lewis94sequential}
\citation{chow:57,chow:70}
\@writefile{toc}{\contentsline {section}{\numberline {2}Unknown Unknowns}{4}}
\newlabel{sec:unknowns}{{2}{4}}
\newlabel{def:ku}{{1}{4}}
\citation{chow:57,chow:70}
\citation{herbei:2005}
\newlabel{eq:rej}{{1}{5}}
\newlabel{eq:costrej}{{2}{5}}
\citation{weiss10disjunct}
\citation{attenberg:2010inactive, attprov:kdd2010}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The reject and classification regions defined by Chow's original classification with a reject option criterion defined in Equation\nobreakspace  {}1\hbox {}}}{6}}
\newlabel{fig:rejectdecision}{{1}{6}}
\newlabel{def:uu}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The reject and classification regions defined by Herbei and Wegkamp's misclassification cost-sensitive classification with a reject option criterion defined in Equation\nobreakspace  {}2\hbox {} for different false negative costs ($\unhbox \voidb@x \hbox {cost}_{\unhbox \voidb@x \hbox {FN}} = \unhbox \voidb@x \hbox {cost}(0|1)$). }}{7}}
\newlabel{fig:costdecision}{{2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Measuring Unknown Unkowns}{8}}
\newlabel{sec:measure}{{3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A typical classification setting. On the the top we see the decision boundary that minimizes the prediction prediction error of a inseparable training set. Additionally, we see the $\epsilon $-radius around the classifier where mistakes are though to occur. The bottom, we see the same classifier with the inclusion of small, disjunctive ``unknowns'', presenting mistakes that occur well outside a model's region of uncertainty. }}{9}}
\newlabel{fig:unknown}{{3}{9}}
\newlabel{fig:150}{{4(a)}{11}}
\newlabel{sub@fig:150}{{(a)}{11}}
\newlabel{fig:1500}{{4(b)}{11}}
\newlabel{sub@fig:1500}{{(b)}{11}}
\newlabel{fig:15000}{{4(c)}{11}}
\newlabel{sub@fig:15000}{{(c)}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces caption of subplots}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$150$ $k$-NN Training Examples}}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$1,500$ $k$-NN Training Examples}}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$15,000$ $k$-NN Training Examples}}}{11}}
\newlabel{fig:uuvsku}{{4}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Beat the Machine}{12}}
\newlabel{sec:btm}{{4}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}BTM Task Design}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A screen-shot of the BTM interface on Mechanical Turk.}}{14}}
\newlabel{fig:btm}{{5}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Studies}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Current and Future Research}{16}}
\citation{Freund99ashort}
\bibstyle{abbrv}
\bibdata{att}
\bibcite{attenberg:2010inactive}{1}
\bibcite{chow:70}{2}
\bibcite{chow:57}{3}
\bibcite{domingos1999metacost}{4}
\bibcite{elkan:2001cost}{5}
\bibcite{Freund99ashort}{6}
\bibcite{herbei:2005}{7}
\bibcite{lewis94sequential}{8}
\bibcite{raykar2009supervised}{9}
\bibcite{Reiter77closedworld}{10}
\bibcite{SettlesActiveLearning}{11}
\bibcite{shengKDD2008}{12}
\bibcite{vanlehn1998analogy}{13}
\bibcite{weiss10disjunct}{14}
\bibcite{winston1970learning}{15}
\newlabel{fig:hate-speech}{{6(a)}{19}}
\newlabel{sub@fig:hate-speech}{{(a)}{19}}
\newlabel{fig:adult}{{6(b)}{19}}
\newlabel{sub@fig:adult}{{(b)}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distributions of the magnitude of the identified mistakes in the predictive model's output by BTM and by random sampling for two ad safety tasks. Each bar indicates the percentage of successfully identified mistakes that reside in the associated score range.}}{19}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Hate Speech}}}{19}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Adult Content}}}{19}}
\newlabel{fig:results}{{5}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learning curves generated by the models using cross-validation (BTM and student lines), and then use as test case for BTM the errors identified by random sampling (BTM on students), and vice versa (students on BTM).}}{20}}
\newlabel{fig:curves}{{7}{20}}
