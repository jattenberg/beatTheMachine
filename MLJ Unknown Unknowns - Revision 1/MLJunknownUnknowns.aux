\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{Reiter77closedworld}
\citation{weiss10disjunct}
\citation{shengKDD2008,raykar2009supervised}
\citation{SettlesActiveLearning}
\citation{winston1970learning}
\citation{vanlehn1998analogy}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Scope}{3}{section.2}}
\citation{elkan:2001cost}
\citation{domingos1999metacost}
\@writefile{toc}{\contentsline {section}{\numberline {3}Unknown Unknowns}{4}{section.3}}
\newlabel{sec:unknowns}{{3}{4}{Unknown Unknowns}{section.3}{}}
\newlabel{equ:expcost}{{1}{4}{Unknown Unknowns}{equation.3.1}{}}
\newlabel{equ:mincost}{{1}{4}{Unknown Unknowns}{equation.3.1}{}}
\citation{lewis94sequential}
\citation{chow:57,chow:70}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The decisions made by a predictive model can be broadly separated into four conceptual regions: (i)\nobreakspace  {}The \emph  {known knowns}, which are the examples for which the model is mostly correct and also returns a low expected misclassification cost (i.e., is also confident of being correct); (ii)\nobreakspace  {}The \emph  {known unknowns}, which are the examples for which the model is often wrong but also anticipates these errors by returning a high expected misclassification cost for these decisions; (iii)\nobreakspace  {}The \emph  {unknown knowns}, which are the examples for which the model is often correct but returns high expected misclassification costs; and (iv)\nobreakspace  {}The \emph  {unknown uknowns}, which are the examples for which the model is wrong but also confident on being correct.}}{5}{figure.1}}
\newlabel{fig:quadrant}{{1}{5}{The decisions made by a predictive model can be broadly separated into four conceptual regions: (i)~The \emph {known knowns}, which are the examples for which the model is mostly correct and also returns a low expected misclassification cost (i.e., is also confident of being correct); (ii)~The \emph {known unknowns}, which are the examples for which the model is often wrong but also anticipates these errors by returning a high expected misclassification cost for these decisions; (iii)~The \emph {unknown knowns}, which are the examples for which the model is often correct but returns high expected misclassification costs; and (iv)~The \emph {unknown uknowns}, which are the examples for which the model is wrong but also confident on being correct}{figure.1}{}}
\newlabel{def:ku}{{1}{5}{Unknown Unknowns}{definition.1}{}}
\citation{weiss10disjunct}
\citation{attenberg:2010inactive}
\newlabel{def:uu}{{2}{6}{Unknown Unknowns}{definition.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A typical classification setting. On the the left we see the decision boundary that minimizes the prediction prediction error of a inseparable training set. Additionally, we see the $\epsilon $-radius around the classifier where mistakes are though to occur. On the right, we see the same classifier with the inclusion of small, disjunctive ``unknowns'', presenting mistakes that occur well outside a model's region of uncertainty. }}{7}{figure.2}}
\newlabel{fig:unknown}{{2}{7}{A typical classification setting. On the the left we see the decision boundary that minimizes the prediction prediction error of a inseparable training set. Additionally, we see the $\epsilon $-radius around the classifier where mistakes are though to occur. On the right, we see the same classifier with the inclusion of small, disjunctive ``unknowns'', presenting mistakes that occur well outside a model's region of uncertainty}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Beat the Machine}{7}{section.4}}
\newlabel{sec:btm}{{4}{7}{Beat the Machine}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Beat the Machine Task Design}{8}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A screen-shot of the BTM interface on Mechanical Turk.}}{9}{figure.3}}
\newlabel{fig:btm}{{3}{9}{A screen-shot of the BTM interface on Mechanical Turk}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Studies}{9}{section.5}}
\newlabel{fig:hate-speech}{{4(a)}{11}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@fig:hate-speech}{{(a)}{11}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{fig:adult}{{4(b)}{11}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@fig:adult}{{(b)}{11}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Distributions of the magnitude of the identified errors by BTM and by random sampling. Each bar indicates the percentage of successfully identified errors that reside in the associated score range.}}{11}{figure.4}}
\newlabel{fig:results}{{5}{11}{Experimental Studies}{figure.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Hate Speech}}}{11}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Adult Content}}}{11}{figure.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Learning curves generated by the models using cross-validation (BTM and student lines), and then use as test case for BTM the errors identified by random sampling (BTM on students), and vice versa (students on BTM).}}{12}{figure.5}}
\newlabel{fig:curves}{{5}{12}{Learning curves generated by the models using cross-validation (BTM and student lines), and then use as test case for BTM the errors identified by random sampling (BTM on students), and vice versa (students on BTM)}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Impact in Industrial Deployments}{12}{section.6}}
\newlabel{sec:impact}{{6}{12}{Impact in Industrial Deployments}{section.6}{}}
\citation{Freund99ashort}
\bibstyle{abbrv}
\bibdata{att}
\bibcite{attenberg:2010inactive}{1}
\bibcite{chow:70}{2}
\bibcite{chow:57}{3}
\bibcite{domingos1999metacost}{4}
\bibcite{elkan:2001cost}{5}
\bibcite{Freund99ashort}{6}
\bibcite{lewis94sequential}{7}
\bibcite{raykar2009supervised}{8}
\bibcite{Reiter77closedworld}{9}
\bibcite{SettlesActiveLearning}{10}
\bibcite{shengKDD2008}{11}
\bibcite{vanlehn1998analogy}{12}
\bibcite{weiss10disjunct}{13}
\bibcite{winston1970learning}{14}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and Future Work}{13}{section.7}}
\newlabel{sec:conclusion}{{7}{13}{Conclusion and Future Work}{section.7}{}}
