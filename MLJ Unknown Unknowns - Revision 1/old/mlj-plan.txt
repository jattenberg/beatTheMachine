Notes on BtM for MLJ special issue.

Idea:

We worked to build classification models for what seemed to be a straightforward and natural application of machine learning: classifying web pages for safe advertising.  An issue that arose was that there were no training data to begin with, and creating training data was particularly challenging due to the low base rate of the positive class(es) and the variety within the positive class.  Consider hate speech.  Previous ML studies have reported difficulties with skews of 10^2 to 10^6.  We are fortunate that hate speech does not occur in 1/10^6 pages on the web.  Elsewhere, we have written about how various existing methods for sampling and active learning failed to obtain satisfactory training data, and proposed methods for dealing with it.  For example <guided learning>.

<Note: finding hate speech actually fits nicely into the "ML for Society" theme, even though the application is ultimately commercial -- so I like it as the running example.>

This paper focuses on a vital part of the problem that arose from working in this application, and to our knowledge has not received any attention in the ML literature: the problem of the "unknown unknowns".    The problem is one of data acquisition and evaluation in realistic settings, and how it is different from the setting usually considered in ML research.  We will elaborate throughout the paper, but in a nutshell the problem is this: in ML research, we assume that the training data are representative and therefore we can use them safely to train and to evaluate our algorithms.  We can know how well our algorithms are performing, and can work to improve them and/or create regions of lower confidence in our predictions.  We have faith in our training data, and we focus on what we can know -- including knowing what we don't know (e.g., regions of uncertainty in our learned model's behavior).

The problem is that for various reasons, the process that produces the training data can completely miss important regions of the space.  Consider a classifier for identifying hate speech online: The category "hate speech" is extremely varied--a "disjunctive concept" in machine learning parlance.  if a certain type of hate speech was not included in the training data, a resultant learned classifier is likely to (i) classify it as not-hate-speech, and worse, (ii) do so with high confidence.  This is an "unknown unknown" -- the classifier does not "know" this sort of hate speech, and it doesn't know that it doesn't know it.  <maybe make a bigger deal of open world/closed world assumption>

This problem may be completely missed by standard ML evaluations: cross-validated error rates and AUCs for such models may look nearly perfect.  However, the machine learning scientist would be incorrect to report to the application stakeholders that the models are essentially perfect.  It is critical to emphasize that the evaluations only consider the known unknowns.  Stakeholders of ML models need also to consider the possible impact of the unknown unknowns (e.g., being blindsided by a client who discovers a completely unknown error).  Moreover, ML scientists and practitioners should work to build tools to help find the unknown unknowns, and turn them into known things.  Finding the unknown unknowns, in a machine learning context, is what this paper is about.

We present a novel framework for thinking about errors of machine learning models that highlights the unknown unknowns (plenty of work has focused on the known unknowns).  We then present a game-structured system (Beat the Machine) that takes advantage of crowdsourcing (more technically, micro-outsourcing) to find the unknown unknowns.  This system is fully implemented at industrial scale; we discuss several design iterations that each presented hurdles to overcome and led to the current system.  Finally we present experimental results demonstrating that on real problems (including finding hate speech pages), explicitly focusing on finding unknown unknowns (with Beat the Machine) indeed finds pages that were completely missed by the prior system, are systematic errors (rather than just isolated outliers), and are systematically different from the prior training data.
